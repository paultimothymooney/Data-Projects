#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 11 15:19:09 2017

@author: ptm
"""

# This code can be used to determine the most optimal parameters for a given classification algorithm.
# Current supports Logistic Regression, Support Vector Machine, and K-Nearest Neighbor algorithms.



import pandas as pd
import numpy as np

# Import libraries for plotting
import matplotlib.pyplot as plt


# Import libraries for machine learning

from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import learning_curve

#from sklearn.metrics import make_scorer, accuracy_score
from sklearn.linear_model import LogisticRegression
#from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
#from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC, LinearSVC
#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier


## LOAD DATA
#
#import os
#os.chdir('/Users/ptm/desktop/Current_working_directory')
## https://www.kaggle.com/uciml/breast-cancer-wisconsin-data
#trainingData = pd.read_csv('data.csv')
#
## SPLIT DATA
#
#X_train = 0
#X_test = 0
#Y_train = 0
#Y_test = 0
#xValues = trainingData.drop(['diagnosis'], axis=1)
#yValues = trainingData['diagnosis']
#X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2, random_state=23)
#
#


##############
##############

# Load the data

import os
os.chdir('/Users/ptm/desktop/Current_working_directory')
data = pd.read_csv('train.csv')
testingData = pd.read_csv('test.csv')


# Limit to 50 images for debugging

X = data.iloc[0:200,1:] # everything but the first column for the first 50 examples (pixel values)
y = data.iloc[0:200,:1] # first column only for the first 50 examples (label/answer)


# You can remove the limit when executing the final script

#X = data.iloc[:,1:] # everything but the first column  (pixel values)
#y = data.iloc[:,:1] # first column only  (label/answer)



##############
##############





# for the Jupyter Notebook I have to run the contents of splitData(input) instead of just calling the function for some reason.
X_train = 0
X_test = 0
Y_train = 0
Y_test = 0
#xValues = trainingData.drop(['diagnosis'], axis=1)
#yValues = trainingData['diagnosis']
#xValues = X
#yValues = y.ravel()
xValues = X
yValues = y.values.ravel()
#yValues = y
X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2, random_state=23)
# End splitData function








# OPTIMIZE CLASSIFIER




from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, accuracy_score

# Optimize Parameters for LR


def selectParametersForLR(a, b, c, d):

    model = LogisticRegression()
    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],
                  'solver' : ['newton-cg', 'lbfgs', 'liblinear']}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer, error_score = 0.01)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for LR:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10, random_state=7)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('Logistic Regression - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForLR(X_train, Y_train, X_test, Y_test)



# Optimize Parameters for SVM

def selectParametersForSVM(a, b, c, d):

    model = SVC()
    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],
                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for SVM:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10, random_state=7)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('Support Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForSVM(X_train, Y_train, X_test, Y_test)



# Optimize Parameters for KNN

def selectParametersForKNN(a, b, c, d):

    model = KNeighborsClassifier()
    parameters = {'n_neighbors': [5, 10, 25, 50],
                  'algorithm': ['ball_tree', 'kd_tree'],
                  'leaf_size': [5, 10, 25, 50]}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for KNN:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10, random_state=7)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('K-Nearest Neighbors Classifier - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForKNN(X_train, Y_train,  X_test, Y_test)





# Answers

#Selected Parameters for LR:
#
#LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,
#          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
#          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,
#          verbose=0, warm_start=False)
#
#Logistic Regression - Training set accuracy: 0.575 (0.296858552176)
#Selected Parameters for SVM:
#
#SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,
#  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
#  max_iter=-1, probability=False, random_state=None, shrinking=True,
#  tol=0.001, verbose=False)
#
#Logistic Regression - Training set accuracy: 0.6 (0.3)
#Selected Parameters for KNN:
#
#KNeighborsClassifier(algorithm='ball_tree', leaf_size=5, metric='minkowski',
#           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#           weights='uniform')
#
#Logistic Regression - Training set accuracy: 0.5 (0.22360679775)

