#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Oct  5 12:29:24 2017

This Python 3 script takes as an input the CSV files from the Kaggle Titanic Dataset (https://www.kaggle.com/c/titanic)
These CSV files contain information on Passenger ID, Ticket Price, Age, Sex, etc.
The 'training data' file contains a column titled 'Survived', while the 'testing data' file does not contain the column titled 'Survived'.
This script uses the aforementioned data to predict whether or not each passenger survived.

@author: ptm
"""



# see jupyter notebook for this script at the following link:
# https://www.kaggle.com/paultimothymooney/titanic-predict-pandas-matplotlib-sklearn/notebook


#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Describe Data 
        #iv.	Make Predictions 
        #v.	Submit Predictions 
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Engineer Data
        #iv.  Select Features 
        #v.	Make Predictions 
        #vi.	Submit Predictions 



#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data


# First we will import two Python libraries that are helpful when analyzing numerical data.  
# We will use "pandas" for the majority of our numerical computations.

import pandas as pd
import numpy as np

# We will also import two Python libraries that are helpful when plotting data.
# We will use matplotlib for the majority of our data visualizations.

import seaborn as sns
import matplotlib.pyplot as plt


# Next I need to set my current working directory to the folder that contains the relevant CSV files.
# These data files were downloaded from https://www.kaggle.com/c/titanic.

import os
os.chdir('/Users/ptm/desktop/Current_working_directory')


# We will begin by loading the relevant data.

trainingData = pd.read_csv('train.csv')
testingData = pd.read_csv('test.csv')


# Next we will inspect the data.  We will try to answer the following questions:
# (1) What are the titles of the columns? ; (2) how many rows are there for each column? ;
# (3) Are there any missing values? (4) What does the data look like?


def describeTheData(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is as follows: (1) the name of each column; (2) the number of values in each column; 
    (3) the number of missing/NaN values in each column; (4) the contents of the first 5 rows; and
    (5) the contents of the last 5 rows.
    """  
    
    print('')
    print('Column Values:')
    print('')
    print(input.columns.values)
    print('')
    print('Value Counts:')
    print('')
    print(input.info())
    print('')
    print('Null Value Counts:')
    print('')
    print(input.isnull().sum())
    print('')
    print('First Few Values:')
    print('')
    print(input.head())
    print('')
    print('Last Few Values:')
    print('')
    print(input.tail())
    print('')
#    print('Descriptive Stats:')
#    print('')
#    print(input.describe())
    return

describeTheData(trainingData)

# Next let's take a closer look at some of the data.
# Perhaps there is a relationship between surivival probability and age?
# Let's find out by plotting the age distributions for passengers that 
# either did or did not survive the sinking of the Titanic.

# %matplotlib inline
# the line above is necessary for the Jupyter Notebook to run properly

def plotAgeDistribution(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is three graphs that each illustrate the distribution of ages for passengers
    that either did or did not survive the sinking of the Titanic.
    """  
    sns.set_style("whitegrid")
    distributionOne = sns.FacetGrid(input, hue="Survived",aspect=2)
    distributionOne.map(plt.hist, 'Age', bins=12)
    distributionOne.add_legend()
    distributionOne.set_axis_labels('Age', 'Count')
    distributionOne.fig.suptitle('Survival Probability vs Age (Blue = Died; Green = Survived)')
    distributionTwo = sns.FacetGrid(input, hue="Survived",aspect=2)
    distributionTwo.map(sns.kdeplot,'Age',shade= True)
    distributionTwo.set(xlim=(0, input['Age'].max()))
    distributionTwo.add_legend()
    distributionTwo.set_axis_labels('Age', 'Proportion')
    distributionTwo.fig.suptitle('Survival Probability vs Age (Blue = Died; Green = Survived)')

    return

plotAgeDistribution(trainingData)




# Hmm... it looks like passengers that were less than five years old were much more likely
# to have survived, but maybe there is not much of a correlation for any other age group.
# As you can see, it is a bit tricky to extract meaning from all of this data.
# Maybe it would be easier to understand if we started out with a much smaller dataset?
# For the sake of simplicity, we are going to delete a bunch of columns, leaving us with 
# only the core of our dataset.  Don't worry, we will replace these missing features later, 
# in addition to also creating some new features.



trainingData = trainingData.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Embarked', 'Cabin'], axis=1)
testingData = testingData.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Embarked', 'Cabin'], axis=1)



#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data
        #ii.	Process Data


# While we are pre-processing our data, we also want to replace any missing values with median values.


def replaceMissingValuesWithMedianValues(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a modified dataframe where all missing values are replaced with median values. 
    """  
    input['Fare'].fillna(input['Fare'].dropna().median(), inplace=True)   
    input['Age'].fillna(input['Age'].dropna().median(), inplace=True)
    return

replaceMissingValuesWithMedianValues(trainingData)
replaceMissingValuesWithMedianValues(testingData)

# Furthermore, we will want to convert all categorical data to a numerical form in order to make our data
# more compatible with the classification algorithms that we will use later on.  In this example, we will 
# convert all of our data into numerical values that are less than five (i.e. 0,1,2,3,4). 


def sexToBinary(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a modified dataframe where 0 = "female" and 1 = "male".
    """ 
    input["Sex"] = input["Sex"].astype("category")
    input["Sex"].cat.categories = [0,1]
    input["Sex"] = input["Sex"].astype("int")
    return

sexToBinary(trainingData)
sexToBinary(testingData)


def ageToCategory(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a modified dataframe where 0 = "ages between 0 and 4", 1 = "ages between 4 and 12",
    2 = "ages between 12 and 18", 3 = "ages between 18 and 60", and 4 = "ages between 60 and 150".
    """ 
    
    input['Age'] = input.Age.fillna(-0.5)
    bins = (-1, 4, 12, 18, 60, 150)
    categories = pd.cut(input.Age, bins, labels=False)
    input.Age = categories
    return

ageToCategory(trainingData)
ageToCategory(testingData)



def fareToCategory(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a modified dataframe where 0 = "ticket price < $10", 1 = "$10<X<$20", 2 = "$20<X<$30", 
    and 3 = "ticket price > $30".
    """ 
    
    input['Fare'] = input.Fare.fillna(-0.5)
    bins = (-1, 10, 20, 30, 1000)
    categories = pd.cut(input.Fare, bins, labels=False)
    input.Fare = categories
    return

fareToCategory(trainingData)
fareToCategory(testingData)


#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Describe Data 


# Now that we have pre-processed our data, let's take a look at it once again.  We want to answer the
# following questions: (1) What are the titles of the columns that we did not delete?; 
# (2) What does the data look like?; (3) Are there any missing values?

def describeDataAgain(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is as follows: (1) the name of each column; (2) the contents of the first 5 rows; and
    (3) the number of missing/NaN values in each column; 
    """ 
    
    print('New summary of data after making changes:')
    print('')
    print('Column Values:')
    print('')
    print(input.columns.values)
    print('')
    print('First Few Values:')
    print('')
    print(input.head())
    print('')
    print('Null Value Counts:')
    print('')
    print(input.isnull().sum())
    return

describeDataAgain(trainingData)


# As you can see, this newly processed data is much easier to work with.  
# Next we will visualize the data by making a heatmap. 
# The heatmap will illustrate the relationship between each numerical feature.  

def makeAHeatMap(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a heatmap showing the relationship between each numerical feature; 
    """  
    
    plt.figure(figsize=[8,6])
    heatmap = sns.heatmap(input.corr(), vmax=1.0, square=True, annot=True)
    heatmap.set_title('Pearson Correlation Coefficients')

    return
    
makeAHeatMap(trainingData)


# As you can see, it looks like there is a pretty good correlation between surivival probability
# and ticket price, ticket class, and gender.  Let's explore this in more detail.


def pivotTheData(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a couple of pivot tables showing the relationship between survival probability
    and each selected feature (i.e. ticket price, ticket class, and gender).
    """    
    
    print('')
    print('Pivot Tables:')
    print('')
    print(input[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))
    print('')
    print(input[["Fare", "Survived"]].groupby(['Fare'], as_index=False).mean().sort_values(by='Survived', ascending=False))
    print('')
    print(input[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))
    print('')
    return

pivotTheData(trainingData)


# Yep, it looks like ticket price, ticket class, and gender are all correlated with survival probability.
# We will use graphs and plots to further illustrate these relationships.



def plotTheData(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is as follows: (1) survival probability vs gender; (2) survival probability vs ticket class; 
    and (3) survival probability vs gender vs ticket class.
    """  
    
    plt.figure(figsize=[10,6])
    plt.subplot(221)
    plotOne = sns.barplot('Sex', 'Survived', data=input, capsize=.1)
    plotOne.set_title('Survival Probability vs Gender (Blue=Female, Green=Male)')
    plt.subplot(222)
    plotTwo = sns.barplot('Pclass', 'Survived', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=".2", edgecolor=".2")
    plotTwo.set_title('Survival Probability vs Ticket Class')

plotTheData(trainingData)


# We can see here that women have a higher probability of survival as compared to men.
# Similarly, passengers with First Class tickets have a higher probability of surivival than those without.
# Now let's look at both variables at the same time.


def plotTheDataAgain(input):
    """ 
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is as follows: (1) survival probability vs gender; (2) survival probability vs ticket class; 
    and (3) survival probability vs gender vs ticket class.
    """  
    
    plt.figure(figsize=[8,5])
    plotThree = sns.pointplot(x="Pclass", y="Survived", hue="Sex", data=input,
                  palette={1: "green", 0: "blue"},
                  markers=["*", "o"], linestyles=["-", "--"]);
    plotThree.set_title('Survival Probability vs Gender vs Ticket Class (Blue=Female, Green=Male)')

plotTheDataAgain(trainingData)


#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Describe Data 
        #iv.	Make Predictions 
        
        

# It looks like the passengers with the highest probability of survival were
# female passengers with First Class tickets.  
# Great!  This means that our classification algorithms should have something
# good to work with.  Next we will identify a suitable classification algorithm
# that we can use to predict whether or not a given passenger might survive.



# To do this, we will import some additional Python libraries that contain
# methods and algorithms that are helpful for machine learning applications.

from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import learning_curve

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier



# Furthermore, we will need to split up our training data, setting aside 20%
# of the training data for cross-validation testing, such that we can avoid
# potentially overfitting the data.


def splitData(input):
    """
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is that 20% of the training data is set aside for cross-validation testing.
    In doing this, it transforms the two arrays (xValues, yValues) into four arrays (X_train, X_test, Y_train, and Y_test).
    """
    X_train = 0
    X_test = 0
    Y_train = 0
    Y_test = 0
    xValues = input.drop(['Survived'], axis=1)
    yValues = input['Survived']
    X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2)
    return

splitData(trainingData)


# for the Jupyter Notebook I have to run the contents of splitData(input) instead of just calling the function for some reason.
X_train = 0
X_test = 0
Y_train = 0
Y_test = 0
xValues = trainingData.drop(['Survived'], axis=1)
yValues = trainingData['Survived']
X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2)
# End splitData function


# The train_test_split function is important for cross-validation
# Learn more about this important concept at the following links:
#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
#http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation


# There are a lot of different classification algorithms to choose between.
# Let's compare nine of them.



def compareABunchOfDifferentModelsAccuracy(a,b,c,d):
    """
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a table and boxplot illustrating the accuracy score for each of nine algorithms given this input.
    """    

    print('')
    print('Compare Multiple Classifiers:')
    print('')
    print('K-Fold Cross-Validation Accuracy:')
    print('')
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('RF', RandomForestClassifier()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('SVM', SVC()))
    models.append(('LSVM', LinearSVC()))
    models.append(('GNB', GaussianNB()))
    models.append(('DTC', DecisionTreeClassifier()))
    models.append(('GBC', GradientBoostingClassifier()))
    #models.append(('LDA', LinearDiscriminantAnalysis()))
        
    resultsAccuracy = []
    names = []
    for name, model in models:
        model.fit(a, b)
        kfold = model_selection.KFold(n_splits=10)
        accuracy_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
        resultsAccuracy.append(accuracy_results)
        names.append(name)
        accuracyMessage = "%s: %f (%f)" % (name, accuracy_results.mean(), accuracy_results.std())
        print(accuracyMessage)

    
    # boxplot algorithm comparison
    fig = plt.figure()
    fig.suptitle('Algorithm Comparison: Accuracy')
    ax = fig.add_subplot(111)
    plt.boxplot(resultsAccuracy)
    ax.set_xticklabels(names)
    ax.set_ylabel('Cross-Validation: Accuracy Score')
    plt.show()
    return



compareABunchOfDifferentModelsAccuracy(X_train, Y_train, X_test, Y_test)


def defineModels():
    """
    This function just defines each abbreviation used in the previous function (e.g. LR = Logistic Regression)
    """
    print('')
    print('LR = LogisticRegression')
    print('RF = RandomForestClassifier')
    print('KNN = KNeighborsClassifier')
    print('SVM = Support Vector Machine SVC')
    print('LSVM = LinearSVC')
    print('GNB = GaussianNB')
    print('DTC = DecisionTreeClassifier')
    print('GBC = GradientBoostingClassifier')
    #print('LDA = LinearDiscriminantAnalysis')
    print('')
    return

defineModels()



# It looks like all nine of these algorithms can do a decent job at this classification task.
# Here we are looking at the "Accuracy Score".  But there is another metric called the F1
# score that does an even better job of comparing model performance.  Let's try that now.


def compareABunchOfDifferentModelsF1Score(a,b,c,d):
    """
    This function takes as an input the dataframe "trainingData" which contains the data from "train.csv".  
    The output is a table and boxplot illustrating the F1 score for each of nine algorithms given this input.
    """   

    print('')
    print('Compare Multiple Classifiers:')
    print('')
    print('F1 Score:')
    print('')
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('RF', RandomForestClassifier()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('SVM', SVC()))
    models.append(('LSVM', LinearSVC()))
    models.append(('GNB', GaussianNB()))
    models.append(('DTC', DecisionTreeClassifier()))
    models.append(('GBC', GradientBoostingClassifier()))
    #models.append(('LDA', LinearDiscriminantAnalysis()))
        
    resultsF1 = []
    names = []
    for name, model in models:
        model.fit(a, b)
        kfold = model_selection.KFold(n_splits=10)
        f1_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='f1_macro')
        resultsF1.append(f1_results)
        names.append(name)
        f1Message = "%s: %f (%f)" % (name, f1_results.mean(), f1_results.std())
        print(f1Message)
        
    fig = plt.figure()
    fig.suptitle('Algorithm Comparison: F1 Score')
    ax = fig.add_subplot(111)
    plt.boxplot(resultsF1)
    ax.set_xticklabels(names)
    ax.set_ylabel('Cross-Validation: F1 Score')
    plt.show()
    return

compareABunchOfDifferentModelsF1Score(X_train, Y_train, X_test, Y_test)
defineModels()


# Again, it looks like all nine algorithms do a pretty decent job.
# Let's look at three of them in more detail.
# Logistic Regression is my favorite algorithm, so let's look at that.
# I also like Support Vector Machines, so we will look at that as well.
# The KNN Classifier also had very good F1 scores so we will look at it too.
# The way that we are going to further compare these three algorithms
# is by looking at the effect of the sample size on the accuracy score
# for both the training dataset and the cross-validation dataset.
# For more information about learning curves, read the following documentation: 
# http://scikit-learn.org/stable/modules/learning_curve.html




def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html
    """
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt


plot_learning_curve(LogisticRegression(), 'Learning Curve For Logistic Regression Classifier', X_train, Y_train, (0.75,0.95), 10)
plot_learning_curve(KNeighborsClassifier(), 'Learning Curve For K Neighbors Classifier', X_train, Y_train, (0.75,0.95), 10)
plot_learning_curve(SVC(), 'Learning Curve For SVM Classifier', X_train, Y_train, (0.75,0.95), 10)

# Great!  These learning curves were really informative.  It looks like maybe the 
# Logistic Regression is overfitting the data.  And the KNN Classifier
# maybe needs an even larger sample size before the training curve and cross-validation
# curve are ready to converge.  It looks like maybe the Support Vector Machine
# algorithm is the best classifier to use for this application.  The learning curve
# you see here for the Support Vector Machine suggests that we do not suffer too much
# from either overfitting or bias.



# So now let's run the Support Vector Machine Classifier
# But first, we should try to optimize the parameters for the SVM.
# We will do it for Logistic Regression and KNN as well.


# To select parameters, we use the functino grid_searchCV
# To learn more about this function, see the following documentation:
# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
#http://scikit-learn.org/stable/modules/grid_search.html#grid-search

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, accuracy_score


def selectParametersForLR(a, b, c, d):

    model = LogisticRegression()
    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],
                  'solver' : ['newton-cg', 'lbfgs', 'liblinear']}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer, error_score = 0.01)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for LR:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('Logistic Regression - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForLR(X_train, Y_train, X_test, Y_test)

# Optimize Parameters for SVM

def selectParametersForSVM(a, b, c, d):

    model = SVC()
    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],
                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for SVM:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('Linear Support Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForSVM(X_train, Y_train, X_test, Y_test)


# Optimize Parameters for KNN

def selectParametersForKNN(a, b, c, d):

    model = KNeighborsClassifier()
    parameters = {'n_neighbors': [5, 10, 25, 50],
                  'algorithm': ['ball_tree', 'kd_tree'],
                  'leaf_size': [5, 10, 25, 50]}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for KNN:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('K-Nearest Neighbors Classifier - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForKNN(X_train, Y_train,  X_test, Y_test)


# That is pretty good for such a simple approach!


#	TABLE OF CONTENTS
    #a.	Part One: 75% Accuracy with a Minimal Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Describe Data 
        #iv.	Make Predictions 
        #v.	Submit Predictions 

# To submit a predictino to kaggle.com, do the following:

# Submission with LinearSVC

# Load testing Data (to extract PassengerID Only)
testingData2 = pd.read_csv('test.csv')
# Define Model, Predict, Submitmodel = votingC
model = LinearSVC(C=0.0001, class_weight=None, dual=True, fit_intercept=True,intercept_scaling=1, loss='squared_hinge', max_iter=1000,multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,verbose=0)
model.fit(X_train, Y_train)
prediction = model.predict(testingData)
prediction = prediction.astype(int)
submission = pd.DataFrame({
    "PassengerId": testingData2["PassengerId"],
    "Survived": prediction})
submission.to_csv('_new_submission_simpleModel.csv', index=False)

# to finish the submission process, upload the file '_new_submission_.csv' to Kaggle





# But remember in the beginning I deleted the majority of the data to makes things easier to understand?
# Now let's start over using the full dataset.  This way we can get an even higher score.
# The other thing that I am going to do is I am going to engineer a few new features,
# and I am going to build a few neural networks.

#	TABLE OF CONTENTS
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data


###
###
###     NEW APPROACH !!!!
###
###

# Load and process data and engineer new features

# Load the data
trainingData = pd.read_csv('train.csv')
testingData = pd.read_csv('test.csv')
# Combine the data so that we can do feature engineering to both the training and testing datasets
trainingData['is_test'] = 0 # this will be helpful when we split up the data again later
testingData['is_test'] = 1 
fullData = pd.concat((trainingData, testingData), axis=0)
# Drop irrelevant data
fullData = fullData.drop(['PassengerId'], axis=1)
# Replace missing values with median values
def replaceMissingWithMedian(dataframe):
    """ Replace missing values wiht median values"""
    dataframe['Fare'].fillna(fullData['Fare'].dropna().median(), inplace=True)   
    dataframe['Age'].fillna(fullData['Age'].dropna().median(), inplace=True)
    dataframe['Cabin'].fillna('Z', inplace=True)
    dataframe['Embarked'].fillna('S', inplace=True)
    dataframe = dataframe.fillna(-0.5)
replaceMissingWithMedian(fullData)
# Replace age numbers with age categories
def replaceAgeNumbersWithCategories(dataframe):
    """Replace age numbers with age categories"""
    dataframe['Age'] = dataframe.Age.fillna(-0.5)
    bins = (-0.01, 4, 12, 18, 60, 150)
    categories = pd.cut(dataframe.Age, bins, labels=False)
    dataframe.Age = categories
replaceAgeNumbersWithCategories(fullData)
# Replace fare numbers with fare categories
def replaceFareNumbersWithCategories(dataframe):
    """Replace fare numbers with fare categories"""
    dataframe['Fare'] = dataframe.Fare.fillna(-0.5)
    bins = (-0.01, 10, 20, 30, 1000)
    categories = pd.cut(dataframe.Fare, bins, labels=False)
    dataframe.Fare = categories
replaceFareNumbersWithCategories(fullData)   

#	TABLE OF CONTENTS
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Engineer Data


# Here we are engineering new features that were not included in our original strategy
# Make feature "Name Title"
def makeFeatureNameTitle(dataframe):
    """ make feature Name Title from out feature Name"""
    name = dataframe['Name']
    full_name = name.str.split(', ', n=0, expand=True)
    last_name = full_name[0]
    titles = full_name[1].str.split('.', n=0, expand=True)
    titles = titles[0]
    dataframe['Name'] = titles
    newTitles = titles.replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    dataframe['Name'] = newTitles
makeFeatureNameTitle(fullData)
# Extract Cabin Letter from Cabin Name and Delete all other info
def extractCabinLetterDeleteOther(dataframe):
    """ Extract Cabin Letter from Cabin Name and Delete all other info"""
    dataframe['Cabin'] = dataframe['Cabin'].str[0]    
    # Extract Family Size from SibSp + Parch and then make new family size groups
    dataframe['FamilySize'] = dataframe['SibSp'] + dataframe['Parch'] + 1
    dataframe['FamilySize'] = dataframe.FamilySize.fillna(-0.5)
    bins = (-1, 1, 2, 4, 6, 1000)
    categories = pd.cut(dataframe.FamilySize, bins, labels=False)
    dataframe['FamilySize'] = categories
extractCabinLetterDeleteOther(fullData)
# Extract whether or not the Ticket Number has a Prefix (possibly indicating special privileges)
def extractTicketPrefix(dataframe):
    """Extract whether or not the Ticket Number has a Prefix (possibly indicating special privileges)"""
    Ticket = []
    for i in list(dataframe.Ticket):
        if not i.isdigit() :
            Ticket.append("1")
        else:
            Ticket.append("0")     
    dataframe["Ticket"] = Ticket
extractTicketPrefix(fullData)


# Replace categorical with numerical
# implements the pd.get_dummies function to convert categorical columns to multiple binary columns
# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html
fullData = pd.get_dummies(fullData, columns=['Pclass', 'Sex', 'Embarked', 'Age', 'Fare', 'Cabin', 'Name', 'FamilySize', 'Ticket'])
# Now we split the combined data back into training and testing data since we have finished with the feature engineering
trainingData = fullData[fullData['is_test'] == 0]
testingData = fullData[fullData['is_test'] == 1]


# Describe the data
describeDataAgain(trainingData)

# Split up the data into K Folds for cross-validation
X = trainingData.drop(['Survived', 'is_test'], axis=1)
y = trainingData['Survived']
#X = trainingData.iloc[0:5000,1:] 
#y = trainingData.iloc[0:5000,:1] 
#y = pd.get_dummies(y)
xValues = X
yValues = y.values.ravel()
#yValues = y
# Here comes the important part
X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2)


# Train and cross-validate multiple classification algorithms and compare the result
# Compare Classification Algorithms
compareABunchOfDifferentModelsAccuracy(X_train, Y_train, X_test, Y_test)
defineModels()



# Now let's plot a bunch of learning curves to see if we can find a method that does not overfit the data
# http://scikit-learn.org/stable/modules/learning_curve.html
def plotLotsOfLearningCurves(a,b):
    """Now let's plot a bunch of learning curves
    # http://scikit-learn.org/stable/modules/learning_curve.html
    """
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('RF', RandomForestClassifier()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('SVM', SVC()))
    models.append(('LSVM', LinearSVC()))
    #models.append(('GNB', GaussianNB()))
    #models.append(('DTC', DecisionTreeClassifier()))
    models.append(('GBC', GradientBoostingClassifier()))
    #models.append(('LDA', LinearDiscriminantAnalysis()))
    
    for name, model in models:
        plot_learning_curve(model, 'Learning Curve For %s Classifier'% (name), a,b, (0.75,0.95), 10)

plotLotsOfLearningCurves(X_train, Y_train)

# Great!  It looks like our best choice to avoid overfitting is: LinearSVC



#	TABLE OF CONTENTS 
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Engineer Data
        #iv.  Select Features 




# Next we will do some feature selection


# How many features should I eliminate?

import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.datasets import make_classification

def determineOptimalFeatureNumber(a,b):
    """
    #http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html
    """
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('RF', RandomForestClassifier()))
    #models.append(('KNN', KNeighborsClassifier()))
    #models.append(('SVM', SVC()))
    models.append(('LSVM', LinearSVC()))
    #models.append(('GNB', GaussianNB()))
    models.append(('DTC', DecisionTreeClassifier()))
    models.append(('GBC', GradientBoostingClassifier()))
    #models.append(('LDA', LinearDiscriminantAnalysis()))
    
    for name, model in models:
        # Create the RFE object and compute a cross-validated score.
        currentModel = model
        # The "accuracy" scoring is proportional to the number of correct
        # classifications
        rfecv = RFECV(estimator=currentModel, step=1, cv=StratifiedKFold(2), scoring='accuracy')
        rfecv.fit(a,b)
        print("Optimal number of features : %d" % rfecv.n_features_)
        # Plot number of features VS. cross-validation scores
        plt.figure()
        plt.xlabel("Number of features selected for %s" % (name))
        plt.ylabel("Cross validation score (nb of correct classifications)")
        plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
        plt.show()

determineOptimalFeatureNumber(X_train, Y_train)


# So right now I have almost 45 features but most algorithms prefer somewhere between 5 and 20 features.
# LinearSVM likes somewhere between 5 and 10.



# Now we will eliminate all but the most helpful features
# We will do this by fitting a LinearSVC and then identifying the best coefficients
#from sklearn.model_selection import GridSearchCV
#from sklearn.metrics import make_scorer, accuracy_score
  
#Run LinearSVC
def runLinearSVC(a,b,c,d):
    """Run LinearSVC w/ Kfold CV"""
    model = LinearSVC()
    model.fit(a,b)
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c,d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('LinearSVC - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
runLinearSVC(X_train, Y_train, X_test, Y_test)


# Identify best feature coefficients (coef_) and/or feature importance (feature_importances_)
model = LinearSVC()
model.fit(X_train,Y_train) # Needed to initialize coef_
columns = X_train.columns
coefficients = model.coef_.reshape(X_train.columns.shape[0], 1)
absCoefficients = abs(coefficients)
fullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)
print('LinearSVC - Feature Importance:')
print('')
print(fullList)
print('')
# Remove all but the most helpful features
topTwenty = fullList[:15]
featureList = topTwenty.values
featureList = pd.DataFrame(featureList)
featuresOnly = featureList[0]
featuresOnly = list(featuresOnly)
featuresOnly += ['is_test', 'Survived']
fullData = fullData[featuresOnly]
trainingData = fullData[fullData['is_test'] == 0]
testingData = fullData[fullData['is_test'] == 1]
#g = sns.heatmap(trainingData[featuresOnly].corr(),cmap="BrBG",annot=False)

# Let's see if we improved our accuracy scores
# Split up the data into K Folds for cross-validation
X = trainingData.drop(['Survived', 'is_test'], axis=1)
y = trainingData['Survived']

#X = trainingData.iloc[0:5000,1:] 
#y = trainingData.iloc[0:5000,:1] 
#y = pd.get_dummies(y)
xValues = X
yValues = y.values.ravel()
#yValues = y
# Here comes the important part
X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2)

print('')
print('Dataset reduced to the following columns:')
print('')
print(X_train.columns)




## See if the score of the Linear SVC improved after narrowing down the number of features

# Run LSVC
print('')
print('After feature selection:')
print('')
runLinearSVC(X_train, Y_train, X_test, Y_test)


# And now let's see how the other classification algorithms like the reduction in features
print('After Feature Selection')
print('')
compareABunchOfDifferentModelsAccuracy(X_train, Y_train, X_test, Y_test)
defineModels()
# Nice that helped a lot!

# We want to use some linear classifiers, so we need to make sure that none of our features are too hgihly correlated
# First let's look at a heatmap of correlations between each feature

g = sns.heatmap(X_train.corr(),cmap="BrBG",annot=False)

# Some of these features are highly correlated 
# This can cause problems for some algorithms such as linear classifiers
# As such, we will now transform our features to make them no longer be correlated
# this is done by applying a transformation and dimensionality reduction to the data
# this process is called principal component analysis (PCA)
# Fore more info, see the following documentaion:
# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
# http://scikit-learn.org/stable/modules/decomposition.html#pca
#

# Now let's do a PCA to help with multicollinearity between the remaining features
from sklearn.decomposition import PCA
# Minimum percentage of variance we want to be described by the resulting transformed components
variance_pct = .99
# Create PCA object
pca = PCA(n_components=variance_pct)
# Transform the initial features
X_transformed = pca.fit_transform(X,y)
#X_transformedTest = pca.fit_transform(xValuesTest,y)
#testingData = testingData[X_transformedTest]
# Create a data frame from the PCA'd data
pcaDataFrame = pd.DataFrame(X_transformed) 
#print(pcaDataFrame.shape[1], " components describe ", str(variance_pct)[1:], "% of the variance")
# Redefine X_train, X_test, Y_train, Y_test
xValues = pcaDataFrame
yValues = y.values.ravel()
#yValues = y
# Here comes the important part
X_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2)
# Now do it to the test data as well
testingData = testingData.drop(['Survived', 'is_test'], axis=1)
testingData = pca.fit_transform(testingData)
testingData = pd.DataFrame(testingData) 

# Now we have new features (we transformed them) with new names
# There are fewer numbers of features now (dimensionality reduction)
# The features are no longer correlated, as illustrated below:

g = sns.heatmap(X_train.corr(),cmap="BrBG",annot=False)


# Alternatively we could have done this:
## First We will eliminate anything with a correlation score greater than .5
#def correlation(dataset, threshold):
#    col_corr = set() # Set of all the names of deleted columns
#    corr_matrix = dataset.corr()
#    for i in range(len(corr_matrix.columns)):
#        for j in range(i):
#            if corr_matrix.iloc[i, j] >= threshold:
#                colname = corr_matrix.columns[i] # getting the name of column
#                col_corr.add(colname)
#                if colname in dataset.columns:
#                    del dataset[colname] # deleting the column from the dataset
#    print('')
#    print('Dataset reduced to the following columns:')
#    print('')
#    print(dataset.columns)
#
#correlation(X_train, 0.5)
#correlation(X_test, 0.5)


#	TABLE OF CONTENTS 
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Engineer Data
        #iv.  Select Features 
        #v.	Make Predictions 


# See if the score of the LinearSVC improved after the Feature Selection + PCA

# Run LSVC
print('')
print('After feature selection + PCA:')
print('')
runLinearSVC(X_train, Y_train, X_test, Y_test)


# Run classifiers after PCA
print('After Feature Selection + PCA')
print('')
compareABunchOfDifferentModelsAccuracy(X_train, Y_train, X_test, Y_test)
defineModels()



# Plot learning curve

plot_learning_curve(LinearSVC(), 'Learning Curve For %s Classifier'% ('LinearSVC'), X_train, Y_train, (0.75,0.95), 10)

# Optimize Parameters for LSVC

# To select parameters, we use the functino grid_searchCV
# To learn more about this function, see the following documentation:
# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
#http://scikit-learn.org/stable/modules/grid_search.html#grid-search

# Optimize Parameters for LSVM

def selectParametersForLSVM(a, b, c, d):

    model = LinearSVC()
    parameters = {'C': [0.00001, 0.001, .01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100, 1000]}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for LSVM:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('Linear Support Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForLSVM(X_train, Y_train, X_test, Y_test)


# Let's also try running a few Neural Networks

# First we will use the Multi-layer Perceptron NN model from Sklearn


from sklearn.neural_network import MLPClassifier as MLPC

def runMLPC(a,b,c,d):
    classifier = MLPC(activation='relu', max_iter=1000)
    classifier.fit(a, b)
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(classifier, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('SKlearn Multi-layer Perceptron NN - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
runMLPC(X_train, Y_train,  X_test, Y_test)

# Now let's see if we can improve the score with parameter optimization

# Optimize Parameters for MLP-NN

def selectParametersForMLPC(a, b, c, d):

    model = MLPC()
    parameters = {'verbose': [False],
                  'activation': ['logistic', 'relu'],
                  'max_iter': [1000, 2000], 'learning_rate': ['constant', 'adaptive']}
    accuracy_scorer = make_scorer(accuracy_score)
    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
    grid_obj = grid_obj.fit(a, b)
    model = grid_obj.best_estimator_
    model.fit(a, b)
    print('Selected Parameters for Multi-Layer Perceptron NN:')
    print('')
    print(model)
    print('')
#    predictions = model.predict(c)
#    print(accuracy_score(d, predictions))
#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
    mean = accuracy.mean() 
    stdev = accuracy.std()
    print('SKlearn Multi-Layer Perceptron - Training set accuracy: %s (%s)' % (mean, stdev))
    print('')
    return

selectParametersForMLPC(X_train, Y_train,  X_test, Y_test)

# Now let's try some other neural networks
from __future__ import print_function
from keras.models import Sequential
from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta
from keras.layers import Dense, Activation, Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from keras.wrappers.scikit_learn import KerasClassifier

from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

# We will build some custom neural networks using keras
# https://keras.io/models/sequential/


def runTwoKerasClassifiers(a,b,c,d):

    global kerasModelOne, kerasModelTwo # eventually I should get rid of these global variables and use classes instead.  in this case i need these variables for the submission function.
    # Let's start out with a simple network consisting of only two fully connected layers.
    Adagrad(lr=0.00001, epsilon=1e-08, decay=0.0)
    model = Sequential()
    model.add(Dense(input_dim=np.array(a).shape[1], units=128, kernel_initializer='normal', bias_initializer='zeros'))
    model.add(Activation('relu'))
    model.add(Dense(units=1))
    model.add(Activation('sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
    model.fit(np.array(a), np.array(b), epochs=10, verbose=2, validation_split=0.2)
      
    
    score = model.evaluate(np.array(c),np.array(d), verbose=0)
    print('')
    print('Loss, Accuracy:')
    print(score)
    
    kerasModelOne = model
    
    # Now let's make a new network, a deep network, that has 15 additional fully connected layers and also 15 dropout functions and we will use more epochs too
    #RMSprop(lr=0.00001, rho=0.9, epsilon=1e-08, decay=0.0)
    Adagrad(lr=0.00001, epsilon=1e-08, decay=0.0)
    #Adadelta(lr=0.00001, rho=0.95, epsilon=1e-08, decay=0.0)
    model = Sequential()
    model.add(Dense(input_dim=np.array(a).shape[1], units=128,
                     kernel_initializer='normal', bias_initializer='zeros'))
    model.add(Activation('relu'))
    for i in range(0, 15):
        model.add(Dense(units=128, kernel_initializer='normal',
                         bias_initializer='zeros'))
        model.add(Activation('relu'))
        model.add(Dropout(.40))
    model.add(Dense(units=1))
    model.add(Activation('sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
    print('Running Deep Neural Network: Expect Delays')
    model.fit(np.array(a), np.array(b), epochs=10, verbose=2, validation_split=0.2)
    
    score = model.evaluate(np.array(c),np.array(d), verbose=0)
    print('')
    print('Loss, Accuracy:')
    print(score)
    
    kerasModelTwo = model
    return kerasModelOne, kerasModelTwo

runTwoKerasClassifiers(X_train,Y_train,X_test,Y_test)


## Hmm it seems like the MLPC Neural Network and the Linear SVM Method both did better than the Keras Neural Network for this particular task.
# To try to get an even higher score, I will now combine the MLPC and LSVM methods by using a new method called ensemble voting.

# Now let's try to do an ensemble voting method.  
# We will let each classifier (MLPC + LVSM) vote for each prediction.
# Note that this method only works with SKlearn classifier

from sklearn.ensemble import VotingClassifier

# To learn more about the VotingClassifier function, see the following documentation:
# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html
# http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier

#votingC = VotingClassifier(estimators=[('LSVM', LinearSVC()), ('MLPC', MLPC()),
#('KerasNN 1', kerasModelOne), ('KerasNN 2',kerasModelTwo)], voting='soft')

def runVotingClassifier(a,b,c,d):
    
    """Run Voting Classifer"""
    global votingC, mean, stdev # eventually I should get rid of these global variables and use classes instead.  in this case i need these variables for the submission function.
    votingC = VotingClassifier(estimators=[('LSVM', LinearSVC(C=0.0001, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0)), ('MLPC', MLPC(activation='logistic', alpha=0.0001, batch_size='auto',
           beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
           hidden_layer_sizes=(100,), learning_rate='constant',
           learning_rate_init=0.001, max_iter=2000, momentum=0.9,
           nesterovs_momentum=True, power_t=0.5, random_state=None,
           shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,
           verbose=False, warm_start=False))], voting='hard')
    
    votingC = votingC.fit(a,b)
    
    kfold = model_selection.KFold(n_splits=10)
    accuracy = model_selection.cross_val_score(votingC, c,d, cv=kfold, scoring='accuracy')
    meanC = accuracy.mean() 
    stdevC = accuracy.std()
    print('Ensemble Voting Method - Training set accuracy: %s (%s)' % (meanC, stdevC))
    print('')
    return votingC, meanC, stdevC
runVotingClassifier(X_train,Y_train,X_test,Y_test)




# It looks like our model can predict with about 80%-85% accuracty whether or not a given
# passenger survived the sinking of the Titanic.  That is pretty good!


# Now let's evaluate our predictions by making a confusion matrix
# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html

# First Make a Prediction

from sklearn.metrics import confusion_matrix
import itertools

model = votingC
model.fit(X_train, Y_train)
prediction = model.predict(X_test)

# To learn more about this confusion matrix, see the following documentation:
# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    # http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Compute confusion matrix
cnf_matrix = confusion_matrix(Y_test, prediction)
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix

class_names = ["Survived", "Did Not Survive"]

plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()



# So it looks like when we make an error, the error tends to be to predict "did not survive" for someone who actually did survive.


# but for now we will submit the answer


#	TABLE OF CONTENTS 
    #b.	Part Two: 80% Accuracy with an Expanded Dataset
        #i.	Load Data
        #ii.	Process Data
        #iii.	Engineer Data
        #iv.  Select Features 
        #v.	Make Predictions 


# Submission with Ensemble Voting Classification Method

# Load testing Data (to extract PassengerID Only)
testingData2 = pd.read_csv('test.csv')
# Define Model, Predict, Submitmodel = votingC
model = votingC
model.fit(X_train, Y_train)
prediction = model.predict(testingData)
prediction = prediction.astype(int)
submission = pd.DataFrame({
    "PassengerId": testingData2["PassengerId"],
    "Survived": prediction})
submission.to_csv('_new_submission_ensemble.csv', index=False)

# to finish the submission process, upload the file '_new_submission_.csv' to Kaggle


#_Alternative Submission for Keras

# Re-Load testing Data (to extract PassengerID Only)
testingData2 = pd.read_csv('test.csv')
# Define Model, Predict, Submit
model = kerasModelTwo
model.fit(np.array(X_train), np.array(Y_train), epochs=50, verbose=2, validation_split=0.2)
prediction = model.predict_classes(np.array(testingData))
submission = pd.DataFrame({"PassengerId": testingData2["PassengerId"], "Survived": prediction.flatten()})
submission.to_csv('_new_submission_Keras.csv', index=False)

# to finish the submission process, upload the file '_new_submission_.csv' to Kaggle

# Submission with LinearSVC

# Load testing Data (to extract PassengerID Only)
testingData2 = pd.read_csv('test.csv')
# Define Model, Predict, Submitmodel = votingC
model = LinearSVC(C=0.0001, class_weight=None, dual=True, fit_intercept=True,intercept_scaling=1, loss='squared_hinge', max_iter=1000,multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,verbose=0)
model.fit(X_train, Y_train)
prediction = model.predict(testingData)
prediction = prediction.astype(int)
submission = pd.DataFrame({
    "PassengerId": testingData2["PassengerId"],
    "Survived": prediction})
submission.to_csv('_new_submission_LinearSVC.csv', index=False)

# to finish the submission process, upload the file '_new_submission_.csv' to Kaggle




##########
##########
##########



#def create_network(a,b):
#    Adagrad(lr=0.00001, epsilon=1e-08, decay=0.0)
#    model = Sequential()
#    model.add(Dense(input_dim=np.array(a).shape[1], units=128, kernel_initializer='normal', bias_initializer='zeros'))
#    model.add(Activation('relu'))
#    model.add(Dense(units=1))
#    model.add(Activation('sigmoid'))
#    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
#    return
#create_network(X_train,Y_train)
#
#
#
#neural_network = KerasClassifier(create_network(X_train,Y_train)))
#cross_val_score(neural_network, X_test, Y_test, cv=3)
#




#def train_and_evaluate__model(a,b,c,d,e):
#    model.fit(np.array(a), np.array(b), epochs=10, verbose=2, validation_split=0.2)   
#    score = model.evaluate(np.array(c),np.array(d), verbose=0)
#    return    
#train_and_evaluate__model(buildModelOne, X_train,Y_train,X_test,Y_test)


#def buildTrainFitModel(a,b,c,d):
#    Adagrad(lr=0.00001, epsilon=1e-08, decay=0.0)
#    model = Sequential()
#    model.add(Dense(input_dim=np.array(a).shape[1], units=128, kernel_initializer='normal', bias_initializer='zeros'))
#    model.add(Activation('relu'))
#    model.add(Dense(units=1))
#    model.add(Activation('sigmoid'))
#    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
#    model.fit(np.array(a), np.array(b), epochs=10, verbose=2, validation_split=0.2)   
#    score = model.evaluate(np.array(c),np.array(d), verbose=0)
#    estimator = KerasClassifier(build_fn=model, nb_epoch=10, batch_size=5, verbose=0)
#    kfold = KFold(n_splits=10)
#    results = cross_val_score(estimator, c,d, cv=kfold)
#    print("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))
##    print('')
##    print('Loss, Accuracy:')
##    print(score)
##    print('')
#buildTrainFitModel(X_train,Y_train,X_test,Y_test)

#n_folds = 10
#skf = StratifiedKFold()
#
#for i, (X_train, Y_train) in enumerate(skf):
#            print("Running Fold", i+1, "/", n_folds)
#            model = None # Clearing the NN.
#            model = create_model()
#            train_and_evaluate_model(model, np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test) )   
#
#
#
#estimator = KerasRegressor(build_fn=buildModelOne(X_train,Y_train), nb_epoch=10, verbose=0)
#kfold = KFold(n_splits=10)
#results = cross_val_score(estimator, X_test, Y_test, cv=kfold)
#print("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))
#






## I will choose to work with the Random Forest Classifier.
## Let's try optimizing the parameters for the RFC
#
#from sklearn import model_selection
#from sklearn.model_selection import train_test_split
#from sklearn.model_selection import learning_curve
#
#from sklearn.linear_model import LogisticRegression
#from sklearn.tree import DecisionTreeClassifier
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
#from sklearn.naive_bayes import GaussianNB
#from sklearn.svm import SVC, LinearSVC
#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
#
#
#from sklearn.metrics import make_scorer, accuracy_score
#
#RFC = RandomForestClassifier()
#accuracy_scorer = make_scorer(accuracy_score)
#parameters = {"max_depth": [None],
#              "max_features": [1, 3, 10],
#              "min_samples_split": [2, 3, 10],
#              "min_samples_leaf": [1, 3, 10],
#              "bootstrap": [False],
#              "n_estimators" :[100,300],
#              "criterion": ["gini"]}
#grid_obj = GridSearchCV(RFC, parameters, cv=kfold, scoring=accuracy_scorer)
#grid_obj = grid_obj.fit(X_train,Y_train)
#RFC_best = grid_obj.best_estimator_
#RFC_best.fit(X_train, Y_train)
#print('Selected Parameters for RandomForestClassifier:')
#print('')
#print(RFC_best)
#print('')
#kfold = model_selection.KFold(n_splits=10, random_state=7)
#accuracy = model_selection.cross_val_score(model, X_test, Y_test, cv=kfold, scoring='accuracy')
#mean = accuracy.mean() 
#stdev = accuracy.std()
#print('K-Nearest Neighbors Classifier - Training set accuracy: %s (%s)' % (mean, stdev))
#print('')







#def runSupportVectorMachine(a, b):
#    """
#    This function takes as an input the four arrays that are generated by the
#    train_test_split function and runs them through a Support Vector Machine classifier.
#    """
#
#    model = SVC()
#    model.fit(a, b)
#    print('')
#    print('Support Vector Machine SVC Classifier:')
#    print('')
#    print('Parameters')
#    print('')
#    print(model)
#    return
#
#runSupportVectorMachine(X_train, Y_train)
#
## Next we will generate an accuracy score and F1 score by using cross-validation with K-Fold
#
#model = SVC()
#
#
#def crossValidateWithKFold(c,d):
#    """
#    This function takes as an input the four arrays that are generated by the
#    train_test_split function and spits out a K-Folds Cross-Validation Accuracy Score
#    """
#
#    kfold = model_selection.KFold(n_splits=10, random_state=7)
#    cv_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')
#    print('')
#    print('K-Fold Cross-Validation Accuracy:')
#    print('')
#    print(cv_results.mean())
#    return
#
#crossValidateWithKFold(X_test, Y_test)
#
#def calculateF1Score(c,d):
#    """
#    This function takes as an input the four arrays that are generated by the
#    train_test_split function and spits out a K-Folds Cross-Validation F1 Score
#    """
#
#    kfold = model_selection.KFold(n_splits=10, random_state=7)
#    cv_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='f1_macro')
#    print('')
#    print('K-Fold F1 Score:')
#    print('')
#    print(cv_results.mean())
#    print('')
#    return
#
#calculateF1Score(X_test, Y_test)






























##########
#########
#########










#### Extra Code ####


# supplementary code #


# Run a logistic regression

#def runLogisticRegression(a, b, c, d):
#
#    model = LogisticRegression()
#    model.fit(a, b)
#    prediction = model.predict(c)
#    print('')
#    print('Logistic Regression:')
#    print('')
#    print('Accuracy:')
#    print(accuracy_score(d, prediction))
#    print('')
#    print('Score:')
#    print(model.score(a, b)) 
#    return
#
#runLogisticRegression(X_train, Y_train, X_test, Y_test)
#
## Cross-validation with K-Fold
#
#def crossValidateWithKFold(input):
#    kfold = model_selection.KFold(n_splits=10, random_state=7)
#    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
#    print('')
#    print('Kfold Cross-Validation Score:')
#    print('')
#    print(cv_results.mean())
#    return
#
#crossValidateWithKFold(trainingData)




# Run a gradient boosting classifier

#def runGradientBoostingClassifier(a, b, c, d):
#
#    model = GradientBoostingClassifier()
#    model.fit(a, b)
#    prediction = model.predict(c)
#    print('')
#    print('Gradient Boosting Classifier:')
#    print('')
#    return

#runGradientBoostingClassifier(X_train, Y_train, X_test, Y_test)


# Now we will run the SVM w/ the selected parameters.



#def runSupportVectorMachine(a, b):
#    """
#    This function takes as an input the four arrays that are generated by the
#    train_test_split function and runs them through a Support Vector Machine classifier.
#    """
#    #model = SVC()
#    model.fit(a, b)
#    print('')
#    print('Support Vector Machine SVC Classifier:')
#    print('')
#    print('Parameters')
#    print('')
#    print(model)
#    return
#
#runSupportVectorMachine(X_train, Y_train)

#from sklearn.model_selection import GridSearchCV
#
#
#def selectParametersForSVM(a, b):
#
#    model = SVC()
#    parameters = {'C': [0.2, 0.4, 0.6, 1, 1.5, 2],
#                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}
#    accuracy_scorer = make_scorer(accuracy_score)
#    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)
#    grid_obj = grid_obj.fit(a, b)
#    model = grid_obj.best_estimator_
#    model.fit(a, b)
#    print('Selected Parameters for SVM:')
#    print('')
#    print(model)
#    print('')
#    return
#
#selectParametersForSVM(X_train, Y_train)


# First, let's add back the column titled "Embarked" which tells us which door the passenger used to board the ship.
# Let's also add back "SibSp" which tells us if there were any siblings.
# And let's also add back "Parch" which tells us if there were any parents.
# We will do this by loading the original data all over agin.

#trainingData = pd.read_csv('train.csv')
#testingData = pd.read_csv('test.csv')
#combinedData = [trainingData, testingData]
#
#trainingData = trainingData.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
#testingData = testingData.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
#
## And again we need to replace missing values with median values and convert categorial data to numerical form. 
#
#replaceMissingValuesWithMedianValues(trainingData)
#replaceMissingValuesWithMedianValues(testingData)
#sexToBinary(trainingData)
#sexToBinary(testingData
#ageToCategory(trainingData)
#ageToCategory(testingData)
#fareToCategory(trainingData)
#fareToCategory(testingData)

# But now we need to process our new columns: Embarked, SibSp, and Parch.

